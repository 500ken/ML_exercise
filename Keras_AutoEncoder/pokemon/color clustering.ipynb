{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Dense\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape= (742, 40, 40, 3)\n",
      "x_test shape= (50, 40, 40, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirPath, dirNames, fileNames in os.walk(\"figure\"):\n",
    "    pass\n",
    "data=[]\n",
    "for name in fileNames:\n",
    "    path='figure/'+name\n",
    "    img=mpimg.imread(path)\n",
    "    data.append(img[:,:,0:3])\n",
    "data=np.array(data)\n",
    "\n",
    "x_train=data[:-50]\n",
    "x_test=data[-50:]\n",
    "\n",
    "x_train=x_train.reshape(len(x_train),40,40,3)\n",
    "x_test=x_test.reshape(len(x_test),40,40,3)\n",
    "\n",
    "print('x_train shape=',x_train.shape)\n",
    "print('x_test shape=',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1267200, 3)\n",
      "(20165, 3)\n"
     ]
    }
   ],
   "source": [
    "hello=data.reshape(-1,3)\n",
    "print(hello.shape)\n",
    "hello=np.unique(hello, axis=0)\n",
    "print(hello.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=100, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "abc=KMeans(100)\n",
    "abc.fit(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22 67 89 ...,  0  0  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.50196081,  0.41960785],\n",
       "       [ 0.        ,  0.54901963,  0.80784315],\n",
       "       ..., \n",
       "       [ 1.        ,  1.        ,  0.99215686],\n",
       "       [ 1.        ,  1.        ,  0.99607843],\n",
       "       [ 1.        ,  1.        ,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(abc.labels_)\n",
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod=data.reshape(-1,3)\n",
    "data_mod=abc.predict(data_mod)\n",
    "data_mod=data_mod.reshape(792,40,40,1)\n",
    "#print(data_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=data_mod[:-50]\n",
    "x_test=data_mod[-50:]\n",
    "\n",
    "x_train=x_train.reshape(len(x_train),40,40,1)\n",
    "x_test=x_test.reshape(len(x_test),40,40,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 40, 40, 8)\n",
      "(?, 20, 20, 8)\n",
      "(?, 20, 20, 8)\n",
      "(?, 10, 10, 8)\n",
      "(?, 10, 10, 4)\n",
      "(?, 2, 2, 4)\n",
      "(?, 2, 2, 4)\n",
      "(?, 10, 10, 4)\n",
      "(?, 10, 10, 8)\n",
      "(?, 20, 20, 8)\n",
      "(?, 20, 20, 8)\n",
      "(?, 40, 40, 8)\n",
      "(?, 40, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "input_img=Input(shape=(40,40,1))\n",
    "\n",
    "x=Conv2D(8,(3,3),activation='relu',padding='same')(input_img)\n",
    "print(x.shape)\n",
    "x=MaxPooling2D((2,2),padding='same')(x)\n",
    "print(x.shape)\n",
    "x=Conv2D(8,(3,3),activation='relu',padding='same')(x)\n",
    "print(x.shape)\n",
    "x=MaxPooling2D((2,2),padding='same')(x)\n",
    "print(x.shape)\n",
    "x=Conv2D(4,(3,3),activation='relu',padding='same')(x)\n",
    "print(x.shape)\n",
    "encoded=MaxPooling2D((5,5),padding='same')(x)\n",
    "print(encoded.shape)\n",
    "\n",
    "x=Conv2D(4,(3,3),activation='relu',padding='same')(encoded)\n",
    "print(x.shape)\n",
    "x=UpSampling2D((5,5))(x)\n",
    "print(x.shape)\n",
    "x=Conv2D(8,(3,3),activation='relu',padding='same')(x)\n",
    "print(x.shape)\n",
    "x=UpSampling2D((2,2))(x)\n",
    "print(x.shape)\n",
    "x=Conv2D(8,(3,3),activation='relu',padding='same')(x)\n",
    "print(x.shape)\n",
    "x=UpSampling2D((2,2))(x)\n",
    "print(x.shape)\n",
    "decoded=Conv2D(1,(3,3),activation='sigmoid',padding='same')(x)\n",
    "print(decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 742 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "742/742 [==============================] - 4s - loss: -5.0721 - val_loss: -50.7936\n",
      "Epoch 2/100\n",
      "742/742 [==============================] - 3s - loss: -27.9247 - val_loss: -181.5236\n",
      "Epoch 3/100\n",
      "742/742 [==============================] - 3s - loss: -73.3373 - val_loss: -319.4681\n",
      "Epoch 4/100\n",
      "742/742 [==============================] - 3s - loss: -89.7622 - val_loss: -353.4538\n",
      "Epoch 5/100\n",
      "742/742 [==============================] - 4s - loss: -90.3014 - val_loss: -360.0184\n",
      "Epoch 6/100\n",
      "742/742 [==============================] - 3s - loss: -90.3259 - val_loss: -361.7768\n",
      "Epoch 7/100\n",
      "742/742 [==============================] - 4s - loss: -90.3051 - val_loss: -362.4026\n",
      "Epoch 8/100\n",
      "742/742 [==============================] - 4s - loss: -90.3003 - val_loss: -362.6564\n",
      "Epoch 9/100\n",
      "742/742 [==============================] - 4s - loss: -90.2969 - val_loss: -362.7637\n",
      "Epoch 10/100\n",
      "742/742 [==============================] - 4s - loss: -90.2945 - val_loss: -362.8146\n",
      "Epoch 11/100\n",
      "742/742 [==============================] - 4s - loss: -90.2941 - val_loss: -362.8255\n",
      "Epoch 12/100\n",
      "742/742 [==============================] - 3s - loss: -90.2940 - val_loss: -362.8238\n",
      "Epoch 13/100\n",
      "742/742 [==============================] - 3s - loss: -90.2943 - val_loss: -362.8117\n",
      "Epoch 14/100\n",
      "742/742 [==============================] - 3s - loss: -90.2948 - val_loss: -362.7923\n",
      "Epoch 15/100\n",
      "512/742 [===================>..........] - ETA: 1s - loss: -92.2872"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6a454d81cb16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\chiang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chiang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chiang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chiang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chiang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chiang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\chiang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chiang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder=Model(input_img,decoded)\n",
    "autoencoder.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train,x_train,epochs=100,batch_size=128,shuffle=True,validation_data=(x_test,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape= (742, 30, 30, 1)\n",
      "x_test shape= (50, 30, 30, 1)\n",
      "Wall time: 638 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "for dirPath, dirNames, fileNames in os.walk(\"figure\"):\n",
    "    pass\n",
    "data=[]\n",
    "for name in fileNames:\n",
    "    path='figure/'+name\n",
    "    img=mpimg.imread(path)\n",
    "    data.append(img[5:-5,5:-5,0])\n",
    "data=np.array(data)\n",
    "\n",
    "x_train=data[:-50]\n",
    "x_test=data[-50:]\n",
    "\n",
    "x_train=x_train.reshape(len(x_train),30,30,1)\n",
    "x_test=x_test.reshape(len(x_test),30,30,1)\n",
    "\n",
    "print('x_train shape=',x_train.shape)\n",
    "print('x_test shape=',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x_train.shape[0]):\n",
    "    x_train[i]=x_train[i]/x_train[i].max()\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_test[i]=x_test[i]/x_test[i].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 30, 30, 16)\n",
      "(?, 10, 10, 16)\n",
      "(?, 10, 10, 16)\n",
      "(?, 5, 5, 16)\n",
      "(?, 5, 5, 16)\n",
      "(?, 10, 10, 16)\n",
      "(?, 10, 10, 16)\n",
      "(?, 30, 30, 16)\n",
      "(?, 30, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "input_img=Input(shape=(30,30,1))\n",
    "\n",
    "x=Conv2D(16,(3,3),activation='relu',padding='same')(input_img)\n",
    "print(x.shape)\n",
    "x=MaxPooling2D((3,3),padding='same')(x)\n",
    "print(x.shape)\n",
    "x=Conv2D(16,(3,3),activation='relu',padding='same')(x)\n",
    "print(x.shape)\n",
    "encoded=MaxPooling2D((2,2),padding='same')(x)\n",
    "print(encoded.shape)\n",
    "\n",
    "x=Conv2D(16,(3,3),activation='relu',padding='same')(encoded)\n",
    "print(x.shape)\n",
    "x=UpSampling2D((2,2))(x)\n",
    "print(x.shape)\n",
    "x=Conv2D(16,(3,3),activation='relu',padding='same')(x)\n",
    "print(x.shape)\n",
    "x=UpSampling2D((3,3))(x)\n",
    "print(x.shape)\n",
    "decoded=Conv2D(1,(3,3),activation='sigmoid',padding='same')(x)\n",
    "print(decoded.shape)\n",
    "\n",
    "autoencoder=Model(input_img,decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 742 samples, validate on 50 samples\n",
      "Epoch 1/500\n",
      "742/742 [==============================] - 3s - loss: 0.2957 - val_loss: 0.2883\n",
      "Epoch 2/500\n",
      "742/742 [==============================] - 2s - loss: 0.2691 - val_loss: 0.2546\n",
      "Epoch 3/500\n",
      "742/742 [==============================] - 2s - loss: 0.2633 - val_loss: 0.2512\n",
      "Epoch 4/500\n",
      "742/742 [==============================] - 2s - loss: 0.2601 - val_loss: 0.2512\n",
      "Epoch 5/500\n",
      "742/742 [==============================] - 2s - loss: 0.2589 - val_loss: 0.2500\n",
      "Epoch 6/500\n",
      "742/742 [==============================] - 2s - loss: 0.2583 - val_loss: 0.2500\n",
      "Epoch 7/500\n",
      "742/742 [==============================] - 2s - loss: 0.2580 - val_loss: 0.2494\n",
      "Epoch 8/500\n",
      "742/742 [==============================] - 2s - loss: 0.2579 - val_loss: 0.2493\n",
      "Epoch 9/500\n",
      "742/742 [==============================] - 2s - loss: 0.2578 - val_loss: 0.2491\n",
      "Epoch 10/500\n",
      "742/742 [==============================] - 2s - loss: 0.2577 - val_loss: 0.2492\n",
      "Epoch 11/500\n",
      "742/742 [==============================] - 2s - loss: 0.2577 - val_loss: 0.2488\n",
      "Epoch 12/500\n",
      "742/742 [==============================] - 2s - loss: 0.2576 - val_loss: 0.2492ss: 0.256\n",
      "Epoch 13/500\n",
      "742/742 [==============================] - 2s - loss: 0.2576 - val_loss: 0.2489ss: 0.25\n",
      "Epoch 14/500\n",
      "742/742 [==============================] - 2s - loss: 0.2574 - val_loss: 0.2487\n",
      "Epoch 15/500\n",
      "742/742 [==============================] - 2s - loss: 0.2574 - val_loss: 0.2487ss: 0.257\n",
      "Epoch 16/500\n",
      "742/742 [==============================] - 2s - loss: 0.2574 - val_loss: 0.2489\n",
      "Epoch 17/500\n",
      "742/742 [==============================] - 2s - loss: 0.2573 - val_loss: 0.2482\n",
      "Epoch 18/500\n",
      "742/742 [==============================] - 2s - loss: 0.2572 - val_loss: 0.2485\n",
      "Epoch 19/500\n",
      "742/742 [==============================] - 2s - loss: 0.2571 - val_loss: 0.2481\n",
      "Epoch 20/500\n",
      "742/742 [==============================] - 2s - loss: 0.2571 - val_loss: 0.2485\n",
      "Epoch 21/500\n",
      "742/742 [==============================] - 2s - loss: 0.2570 - val_loss: 0.2478\n",
      "Epoch 22/500\n",
      "742/742 [==============================] - 2s - loss: 0.2570 - val_loss: 0.2483\n",
      "Epoch 23/500\n",
      "742/742 [==============================] - 2s - loss: 0.2569 - val_loss: 0.2483ss: 0.2\n",
      "Epoch 24/500\n",
      "742/742 [==============================] - 2s - loss: 0.2568 - val_loss: 0.2478\n",
      "Epoch 25/500\n",
      "742/742 [==============================] - 2s - loss: 0.2567 - val_loss: 0.2480\n",
      "Epoch 26/500\n",
      "742/742 [==============================] - 2s - loss: 0.2567 - val_loss: 0.2480\n",
      "Epoch 27/500\n",
      "742/742 [==============================] - 2s - loss: 0.2566 - val_loss: 0.2481\n",
      "Epoch 28/500\n",
      "742/742 [==============================] - 2s - loss: 0.2565 - val_loss: 0.2477\n",
      "Epoch 29/500\n",
      "742/742 [==============================] - 2s - loss: 0.2564 - val_loss: 0.2478\n",
      "Epoch 30/500\n",
      "742/742 [==============================] - 2s - loss: 0.2563 - val_loss: 0.2478ss: \n",
      "Epoch 31/500\n",
      "742/742 [==============================] - 2s - loss: 0.2562 - val_loss: 0.2475\n",
      "Epoch 32/500\n",
      "742/742 [==============================] - 2s - loss: 0.2562 - val_loss: 0.2474\n",
      "Epoch 33/500\n",
      "742/742 [==============================] - 2s - loss: 0.2561 - val_loss: 0.2472\n",
      "Epoch 34/500\n",
      "742/742 [==============================] - 2s - loss: 0.2561 - val_loss: 0.2474\n",
      "Epoch 35/500\n",
      "742/742 [==============================] - 2s - loss: 0.2561 - val_loss: 0.2474\n",
      "Epoch 36/500\n",
      "742/742 [==============================] - 2s - loss: 0.2560 - val_loss: 0.2476\n",
      "Epoch 37/500\n",
      "742/742 [==============================] - 2s - loss: 0.2559 - val_loss: 0.2471\n",
      "Epoch 38/500\n",
      "742/742 [==============================] - 2s - loss: 0.2557 - val_loss: 0.2474\n",
      "Epoch 39/500\n",
      "742/742 [==============================] - 2s - loss: 0.2557 - val_loss: 0.2469\n",
      "Epoch 40/500\n",
      "742/742 [==============================] - 2s - loss: 0.2559 - val_loss: 0.2466\n",
      "Epoch 41/500\n",
      "742/742 [==============================] - 2s - loss: 0.2556 - val_loss: 0.2469\n",
      "Epoch 42/500\n",
      "742/742 [==============================] - 2s - loss: 0.2555 - val_loss: 0.2473\n",
      "Epoch 43/500\n",
      "742/742 [==============================] - 2s - loss: 0.2556 - val_loss: 0.2467\n",
      "Epoch 44/500\n",
      "742/742 [==============================] - 2s - loss: 0.2553 - val_loss: 0.2467\n",
      "Epoch 45/500\n",
      "742/742 [==============================] - 2s - loss: 0.2552 - val_loss: 0.2468\n",
      "Epoch 46/500\n",
      "742/742 [==============================] - 2s - loss: 0.2551 - val_loss: 0.2466\n",
      "Epoch 47/500\n",
      "742/742 [==============================] - 2s - loss: 0.2550 - val_loss: 0.2469\n",
      "Epoch 48/500\n",
      "742/742 [==============================] - 2s - loss: 0.2549 - val_loss: 0.2465\n",
      "Epoch 49/500\n",
      "742/742 [==============================] - 2s - loss: 0.2548 - val_loss: 0.2466\n",
      "Epoch 50/500\n",
      "742/742 [==============================] - 2s - loss: 0.2547 - val_loss: 0.2465\n",
      "Epoch 51/500\n",
      "742/742 [==============================] - 2s - loss: 0.2547 - val_loss: 0.2465\n",
      "Epoch 52/500\n",
      "742/742 [==============================] - 2s - loss: 0.2546 - val_loss: 0.2462\n",
      "Epoch 53/500\n",
      "742/742 [==============================] - 2s - loss: 0.2545 - val_loss: 0.2464\n",
      "Epoch 54/500\n",
      "742/742 [==============================] - 2s - loss: 0.2544 - val_loss: 0.2464\n",
      "Epoch 55/500\n",
      "742/742 [==============================] - 2s - loss: 0.2543 - val_loss: 0.2460\n",
      "Epoch 56/500\n",
      "742/742 [==============================] - 2s - loss: 0.2543 - val_loss: 0.2460\n",
      "Epoch 57/500\n",
      "742/742 [==============================] - 3s - loss: 0.2543 - val_loss: 0.2461\n",
      "Epoch 58/500\n",
      "742/742 [==============================] - 2s - loss: 0.2542 - val_loss: 0.2458\n",
      "Epoch 59/500\n",
      "742/742 [==============================] - 2s - loss: 0.2540 - val_loss: 0.2457\n",
      "Epoch 60/500\n",
      "742/742 [==============================] - 2s - loss: 0.2540 - val_loss: 0.2455\n",
      "Epoch 61/500\n",
      "742/742 [==============================] - 2s - loss: 0.2539 - val_loss: 0.2457\n",
      "Epoch 62/500\n",
      "742/742 [==============================] - 2s - loss: 0.2538 - val_loss: 0.2465\n",
      "Epoch 63/500\n",
      "742/742 [==============================] - 2s - loss: 0.2539 - val_loss: 0.2453\n",
      "Epoch 64/500\n",
      "742/742 [==============================] - 3s - loss: 0.2540 - val_loss: 0.2452\n",
      "Epoch 65/500\n",
      "742/742 [==============================] - 2s - loss: 0.2539 - val_loss: 0.2455\n",
      "Epoch 66/500\n",
      "742/742 [==============================] - 2s - loss: 0.2535 - val_loss: 0.2455\n",
      "Epoch 67/500\n",
      "742/742 [==============================] - 2s - loss: 0.2536 - val_loss: 0.2452\n",
      "Epoch 68/500\n",
      "742/742 [==============================] - 2s - loss: 0.2535 - val_loss: 0.2450\n",
      "Epoch 69/500\n",
      "742/742 [==============================] - 3s - loss: 0.2534 - val_loss: 0.2462\n",
      "Epoch 70/500\n",
      "742/742 [==============================] - 2s - loss: 0.2535 - val_loss: 0.2461\n",
      "Epoch 71/500\n",
      "742/742 [==============================] - 2s - loss: 0.2537 - val_loss: 0.2451\n",
      "Epoch 72/500\n",
      "742/742 [==============================] - 2s - loss: 0.2536 - val_loss: 0.2452\n",
      "Epoch 73/500\n",
      "742/742 [==============================] - 2s - loss: 0.2530 - val_loss: 0.2447\n",
      "Epoch 74/500\n",
      "742/742 [==============================] - 2s - loss: 0.2530 - val_loss: 0.2454\n",
      "Epoch 75/500\n",
      "742/742 [==============================] - 2s - loss: 0.2528 - val_loss: 0.2456\n",
      "Epoch 76/500\n",
      "742/742 [==============================] - 2s - loss: 0.2528 - val_loss: 0.2453\n",
      "Epoch 77/500\n",
      "742/742 [==============================] - 3s - loss: 0.2526 - val_loss: 0.2444\n",
      "Epoch 78/500\n",
      "742/742 [==============================] - 2s - loss: 0.2525 - val_loss: 0.2447\n",
      "Epoch 79/500\n",
      "742/742 [==============================] - 2s - loss: 0.2524 - val_loss: 0.2446\n",
      "Epoch 80/500\n",
      "742/742 [==============================] - 2s - loss: 0.2524 - val_loss: 0.2442\n",
      "Epoch 81/500\n",
      "742/742 [==============================] - 2s - loss: 0.2526 - val_loss: 0.2447\n",
      "Epoch 82/500\n",
      "742/742 [==============================] - 2s - loss: 0.2523 - val_loss: 0.2445ss: 0.\n",
      "Epoch 83/500\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.253 - 2s - loss: 0.2521 - val_loss: 0.2443\n",
      "Epoch 84/500\n",
      "742/742 [==============================] - 2s - loss: 0.2522 - val_loss: 0.2441\n",
      "Epoch 85/500\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.250 - 2s - loss: 0.2524 - val_loss: 0.2440\n",
      "Epoch 86/500\n",
      "742/742 [==============================] - 2s - loss: 0.2523 - val_loss: 0.2443\n",
      "Epoch 87/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 2s - loss: 0.2520 - val_loss: 0.2439\n",
      "Epoch 88/500\n",
      "742/742 [==============================] - 2s - loss: 0.2517 - val_loss: 0.2437\n",
      "Epoch 89/500\n",
      "742/742 [==============================] - 2s - loss: 0.2517 - val_loss: 0.2439\n",
      "Epoch 90/500\n",
      "742/742 [==============================] - 2s - loss: 0.2515 - val_loss: 0.2438\n",
      "Epoch 91/500\n",
      "742/742 [==============================] - 2s - loss: 0.2515 - val_loss: 0.2440\n",
      "Epoch 92/500\n",
      "742/742 [==============================] - 2s - loss: 0.2515 - val_loss: 0.2438\n",
      "Epoch 93/500\n",
      "742/742 [==============================] - 2s - loss: 0.2516 - val_loss: 0.2442\n",
      "Epoch 94/500\n",
      "742/742 [==============================] - 2s - loss: 0.2514 - val_loss: 0.2440\n",
      "Epoch 95/500\n",
      "742/742 [==============================] - 2s - loss: 0.2513 - val_loss: 0.2435\n",
      "Epoch 96/500\n",
      "742/742 [==============================] - 2s - loss: 0.2512 - val_loss: 0.2434\n",
      "Epoch 97/500\n",
      "742/742 [==============================] - 2s - loss: 0.2512 - val_loss: 0.2437\n",
      "Epoch 98/500\n",
      "742/742 [==============================] - 2s - loss: 0.2512 - val_loss: 0.2438\n",
      "Epoch 99/500\n",
      "742/742 [==============================] - 2s - loss: 0.2512 - val_loss: 0.2435\n",
      "Epoch 100/500\n",
      "742/742 [==============================] - 2s - loss: 0.2509 - val_loss: 0.2440\n",
      "Epoch 101/500\n",
      "742/742 [==============================] - 2s - loss: 0.2509 - val_loss: 0.2435\n",
      "Epoch 102/500\n",
      "742/742 [==============================] - 2s - loss: 0.2509 - val_loss: 0.2434\n",
      "Epoch 103/500\n",
      "742/742 [==============================] - 2s - loss: 0.2511 - val_loss: 0.2449\n",
      "Epoch 104/500\n",
      "742/742 [==============================] - 2s - loss: 0.2510 - val_loss: 0.2438\n",
      "Epoch 105/500\n",
      "742/742 [==============================] - 2s - loss: 0.2508 - val_loss: 0.2434\n",
      "Epoch 106/500\n",
      "742/742 [==============================] - 2s - loss: 0.2506 - val_loss: 0.2444\n",
      "Epoch 107/500\n",
      "742/742 [==============================] - 2s - loss: 0.2504 - val_loss: 0.2435\n",
      "Epoch 108/500\n",
      "742/742 [==============================] - 2s - loss: 0.2503 - val_loss: 0.2431ss: \n",
      "Epoch 109/500\n",
      "742/742 [==============================] - 2s - loss: 0.2503 - val_loss: 0.2433\n",
      "Epoch 110/500\n",
      "742/742 [==============================] - 2s - loss: 0.2502 - val_loss: 0.2434\n",
      "Epoch 111/500\n",
      "742/742 [==============================] - 2s - loss: 0.2503 - val_loss: 0.2435\n",
      "Epoch 112/500\n",
      "742/742 [==============================] - 2s - loss: 0.2502 - val_loss: 0.2428\n",
      "Epoch 113/500\n",
      "742/742 [==============================] - 2s - loss: 0.2503 - val_loss: 0.2433\n",
      "Epoch 114/500\n",
      "742/742 [==============================] - 2s - loss: 0.2503 - val_loss: 0.2441\n",
      "Epoch 115/500\n",
      "742/742 [==============================] - 2s - loss: 0.2508 - val_loss: 0.2432\n",
      "Epoch 116/500\n",
      "742/742 [==============================] - 2s - loss: 0.2500 - val_loss: 0.2432\n",
      "Epoch 117/500\n",
      "742/742 [==============================] - 2s - loss: 0.2498 - val_loss: 0.2428\n",
      "Epoch 118/500\n",
      "742/742 [==============================] - 2s - loss: 0.2497 - val_loss: 0.2433\n",
      "Epoch 119/500\n",
      "742/742 [==============================] - 2s - loss: 0.2497 - val_loss: 0.2433\n",
      "Epoch 120/500\n",
      "742/742 [==============================] - 2s - loss: 0.2495 - val_loss: 0.2434\n",
      "Epoch 121/500\n",
      "742/742 [==============================] - 2s - loss: 0.2495 - val_loss: 0.2430\n",
      "Epoch 122/500\n",
      "742/742 [==============================] - 2s - loss: 0.2497 - val_loss: 0.2430\n",
      "Epoch 123/500\n",
      "742/742 [==============================] - 2s - loss: 0.2500 - val_loss: 0.2436\n",
      "Epoch 124/500\n",
      "742/742 [==============================] - 2s - loss: 0.2494 - val_loss: 0.2427\n",
      "Epoch 125/500\n",
      "742/742 [==============================] - 2s - loss: 0.2493 - val_loss: 0.2434\n",
      "Epoch 126/500\n",
      "742/742 [==============================] - 2s - loss: 0.2496 - val_loss: 0.2434\n",
      "Epoch 127/500\n",
      "742/742 [==============================] - 2s - loss: 0.2491 - val_loss: 0.2427\n",
      "Epoch 128/500\n",
      "742/742 [==============================] - 2s - loss: 0.2492 - val_loss: 0.2428\n",
      "Epoch 129/500\n",
      "742/742 [==============================] - 2s - loss: 0.2490 - val_loss: 0.2426\n",
      "Epoch 130/500\n",
      "742/742 [==============================] - 2s - loss: 0.2491 - val_loss: 0.2435\n",
      "Epoch 131/500\n",
      "742/742 [==============================] - 2s - loss: 0.2490 - val_loss: 0.2429\n",
      "Epoch 132/500\n",
      "742/742 [==============================] - 2s - loss: 0.2487 - val_loss: 0.2437\n",
      "Epoch 133/500\n",
      "742/742 [==============================] - 2s - loss: 0.2487 - val_loss: 0.2429\n",
      "Epoch 134/500\n",
      "742/742 [==============================] - 2s - loss: 0.2487 - val_loss: 0.2427\n",
      "Epoch 135/500\n",
      "742/742 [==============================] - 2s - loss: 0.2487 - val_loss: 0.2429\n",
      "Epoch 136/500\n",
      "742/742 [==============================] - 2s - loss: 0.2485 - val_loss: 0.2427\n",
      "Epoch 137/500\n",
      "742/742 [==============================] - 2s - loss: 0.2484 - val_loss: 0.2426ss: 0\n",
      "Epoch 138/500\n",
      "742/742 [==============================] - 2s - loss: 0.2483 - val_loss: 0.2431\n",
      "Epoch 139/500\n",
      "742/742 [==============================] - 2s - loss: 0.2484 - val_loss: 0.2428\n",
      "Epoch 140/500\n",
      "742/742 [==============================] - 2s - loss: 0.2483 - val_loss: 0.2423\n",
      "Epoch 141/500\n",
      "742/742 [==============================] - 2s - loss: 0.2484 - val_loss: 0.2424\n",
      "Epoch 142/500\n",
      "742/742 [==============================] - 2s - loss: 0.2489 - val_loss: 0.2437\n",
      "Epoch 143/500\n",
      "742/742 [==============================] - 2s - loss: 0.2492 - val_loss: 0.2423\n",
      "Epoch 144/500\n",
      "742/742 [==============================] - 2s - loss: 0.2482 - val_loss: 0.2425\n",
      "Epoch 145/500\n",
      "742/742 [==============================] - 2s - loss: 0.2480 - val_loss: 0.2421\n",
      "Epoch 146/500\n",
      "742/742 [==============================] - 2s - loss: 0.2480 - val_loss: 0.2422\n",
      "Epoch 147/500\n",
      "742/742 [==============================] - 2s - loss: 0.2482 - val_loss: 0.2425\n",
      "Epoch 148/500\n",
      "742/742 [==============================] - 2s - loss: 0.2481 - val_loss: 0.2425\n",
      "Epoch 149/500\n",
      "742/742 [==============================] - 2s - loss: 0.2481 - val_loss: 0.2423\n",
      "Epoch 150/500\n",
      "742/742 [==============================] - 2s - loss: 0.2477 - val_loss: 0.2420\n",
      "Epoch 151/500\n",
      "742/742 [==============================] - 2s - loss: 0.2477 - val_loss: 0.2424\n",
      "Epoch 152/500\n",
      "742/742 [==============================] - 2s - loss: 0.2479 - val_loss: 0.2428\n",
      "Epoch 153/500\n",
      "742/742 [==============================] - 2s - loss: 0.2478 - val_loss: 0.2425\n",
      "Epoch 154/500\n",
      "742/742 [==============================] - 2s - loss: 0.2479 - val_loss: 0.2427s\n",
      "Epoch 155/500\n",
      "742/742 [==============================] - 2s - loss: 0.2478 - val_loss: 0.2427\n",
      "Epoch 156/500\n",
      "742/742 [==============================] - 2s - loss: 0.2475 - val_loss: 0.2421\n",
      "Epoch 157/500\n",
      "742/742 [==============================] - 2s - loss: 0.2475 - val_loss: 0.2417\n",
      "Epoch 158/500\n",
      "742/742 [==============================] - 2s - loss: 0.2476 - val_loss: 0.2418\n",
      "Epoch 159/500\n",
      "742/742 [==============================] - 2s - loss: 0.2477 - val_loss: 0.2423\n",
      "Epoch 160/500\n",
      "742/742 [==============================] - 2s - loss: 0.2472 - val_loss: 0.2424\n",
      "Epoch 161/500\n",
      "742/742 [==============================] - 2s - loss: 0.2473 - val_loss: 0.2420\n",
      "Epoch 162/500\n",
      "742/742 [==============================] - 2s - loss: 0.2474 - val_loss: 0.2424\n",
      "Epoch 163/500\n",
      "742/742 [==============================] - 2s - loss: 0.2472 - val_loss: 0.2424\n",
      "Epoch 164/500\n",
      "742/742 [==============================] - 2s - loss: 0.2472 - val_loss: 0.2421\n",
      "Epoch 165/500\n",
      "742/742 [==============================] - 2s - loss: 0.2478 - val_loss: 0.2420\n",
      "Epoch 166/500\n",
      "742/742 [==============================] - 2s - loss: 0.2471 - val_loss: 0.2416s\n",
      "Epoch 167/500\n",
      "742/742 [==============================] - 2s - loss: 0.2471 - val_loss: 0.2425\n",
      "Epoch 168/500\n",
      "742/742 [==============================] - 2s - loss: 0.2471 - val_loss: 0.2424\n",
      "Epoch 169/500\n",
      "742/742 [==============================] - 2s - loss: 0.2469 - val_loss: 0.2416\n",
      "Epoch 170/500\n",
      "742/742 [==============================] - 2s - loss: 0.2468 - val_loss: 0.2425\n",
      "Epoch 171/500\n",
      "742/742 [==============================] - 2s - loss: 0.2473 - val_loss: 0.2415\n",
      "Epoch 172/500\n",
      "742/742 [==============================] - 2s - loss: 0.2468 - val_loss: 0.2419\n",
      "Epoch 173/500\n",
      "742/742 [==============================] - 2s - loss: 0.2469 - val_loss: 0.2425\n",
      "Epoch 174/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 2s - loss: 0.2466 - val_loss: 0.2424\n",
      "Epoch 175/500\n",
      "742/742 [==============================] - 3s - loss: 0.2465 - val_loss: 0.2417\n",
      "Epoch 176/500\n",
      "742/742 [==============================] - 2s - loss: 0.2467 - val_loss: 0.2430\n",
      "Epoch 177/500\n",
      "742/742 [==============================] - 2s - loss: 0.2468 - val_loss: 0.2427\n",
      "Epoch 178/500\n",
      "742/742 [==============================] - 2s - loss: 0.2465 - val_loss: 0.2418\n",
      "Epoch 179/500\n",
      "742/742 [==============================] - 3s - loss: 0.2464 - val_loss: 0.2423\n",
      "Epoch 180/500\n",
      "742/742 [==============================] - 3s - loss: 0.2463 - val_loss: 0.2422\n",
      "Epoch 181/500\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.245 - 2s - loss: 0.2462 - val_loss: 0.2421\n",
      "Epoch 182/500\n",
      "742/742 [==============================] - 2s - loss: 0.2461 - val_loss: 0.2419\n",
      "Epoch 183/500\n",
      "742/742 [==============================] - 2s - loss: 0.2461 - val_loss: 0.2415\n",
      "Epoch 184/500\n",
      "742/742 [==============================] - 2s - loss: 0.2464 - val_loss: 0.2426\n",
      "Epoch 185/500\n",
      "742/742 [==============================] - 2s - loss: 0.2468 - val_loss: 0.2436\n",
      "Epoch 186/500\n",
      "742/742 [==============================] - 2s - loss: 0.2467 - val_loss: 0.2416\n",
      "Epoch 187/500\n",
      "742/742 [==============================] - 2s - loss: 0.2462 - val_loss: 0.2415\n",
      "Epoch 188/500\n",
      "742/742 [==============================] - 2s - loss: 0.2461 - val_loss: 0.2420\n",
      "Epoch 189/500\n",
      "742/742 [==============================] - 2s - loss: 0.2462 - val_loss: 0.2424\n",
      "Epoch 190/500\n",
      "742/742 [==============================] - 2s - loss: 0.2461 - val_loss: 0.2422\n",
      "Epoch 191/500\n",
      "742/742 [==============================] - 2s - loss: 0.2460 - val_loss: 0.2417\n",
      "Epoch 192/500\n",
      "742/742 [==============================] - 2s - loss: 0.2460 - val_loss: 0.2424\n",
      "Epoch 193/500\n",
      "742/742 [==============================] - 2s - loss: 0.2458 - val_loss: 0.2419ss: 0.\n",
      "Epoch 194/500\n",
      "742/742 [==============================] - 2s - loss: 0.2458 - val_loss: 0.2414\n",
      "Epoch 195/500\n",
      "742/742 [==============================] - 2s - loss: 0.2456 - val_loss: 0.2417\n",
      "Epoch 196/500\n",
      "742/742 [==============================] - 2s - loss: 0.2456 - val_loss: 0.2414\n",
      "Epoch 197/500\n",
      "742/742 [==============================] - 2s - loss: 0.2457 - val_loss: 0.2418\n",
      "Epoch 198/500\n",
      "742/742 [==============================] - 2s - loss: 0.2458 - val_loss: 0.2414\n",
      "Epoch 199/500\n",
      "742/742 [==============================] - 2s - loss: 0.2457 - val_loss: 0.2419\n",
      "Epoch 200/500\n",
      "742/742 [==============================] - 2s - loss: 0.2455 - val_loss: 0.2422\n",
      "Epoch 201/500\n",
      "742/742 [==============================] - 2s - loss: 0.2455 - val_loss: 0.2417\n",
      "Epoch 202/500\n",
      "742/742 [==============================] - 2s - loss: 0.2453 - val_loss: 0.2414\n",
      "Epoch 203/500\n",
      "742/742 [==============================] - 2s - loss: 0.2453 - val_loss: 0.2414\n",
      "Epoch 204/500\n",
      "742/742 [==============================] - 2s - loss: 0.2452 - val_loss: 0.2420\n",
      "Epoch 205/500\n",
      "742/742 [==============================] - 2s - loss: 0.2452 - val_loss: 0.2417\n",
      "Epoch 206/500\n",
      "742/742 [==============================] - 2s - loss: 0.2452 - val_loss: 0.2413\n",
      "Epoch 207/500\n",
      "742/742 [==============================] - 2s - loss: 0.2451 - val_loss: 0.2418\n",
      "Epoch 208/500\n",
      "742/742 [==============================] - 2s - loss: 0.2454 - val_loss: 0.2415\n",
      "Epoch 209/500\n",
      "742/742 [==============================] - 2s - loss: 0.2453 - val_loss: 0.2422\n",
      "Epoch 210/500\n",
      "742/742 [==============================] - 2s - loss: 0.2451 - val_loss: 0.2414\n",
      "Epoch 211/500\n",
      "742/742 [==============================] - 2s - loss: 0.2451 - val_loss: 0.2418\n",
      "Epoch 212/500\n",
      "742/742 [==============================] - 2s - loss: 0.2450 - val_loss: 0.2414\n",
      "Epoch 213/500\n",
      "742/742 [==============================] - 2s - loss: 0.2450 - val_loss: 0.2413\n",
      "Epoch 214/500\n",
      "742/742 [==============================] - 2s - loss: 0.2449 - val_loss: 0.2426\n",
      "Epoch 215/500\n",
      "742/742 [==============================] - 2s - loss: 0.2452 - val_loss: 0.2411\n",
      "Epoch 216/500\n",
      "742/742 [==============================] - 2s - loss: 0.2450 - val_loss: 0.2418\n",
      "Epoch 217/500\n",
      "742/742 [==============================] - 2s - loss: 0.2451 - val_loss: 0.2415\n",
      "Epoch 218/500\n",
      "742/742 [==============================] - 2s - loss: 0.2449 - val_loss: 0.2411\n",
      "Epoch 219/500\n",
      "742/742 [==============================] - 2s - loss: 0.2451 - val_loss: 0.2420\n",
      "Epoch 220/500\n",
      "742/742 [==============================] - 2s - loss: 0.2447 - val_loss: 0.2408\n",
      "Epoch 221/500\n",
      "742/742 [==============================] - 2s - loss: 0.2447 - val_loss: 0.2411\n",
      "Epoch 222/500\n",
      "742/742 [==============================] - 2s - loss: 0.2445 - val_loss: 0.2410\n",
      "Epoch 223/500\n",
      "742/742 [==============================] - 2s - loss: 0.2445 - val_loss: 0.2415\n",
      "Epoch 224/500\n",
      "742/742 [==============================] - 2s - loss: 0.2448 - val_loss: 0.2409\n",
      "Epoch 225/500\n",
      "742/742 [==============================] - 2s - loss: 0.2448 - val_loss: 0.2414\n",
      "Epoch 226/500\n",
      "742/742 [==============================] - 2s - loss: 0.2445 - val_loss: 0.2417\n",
      "Epoch 227/500\n",
      "742/742 [==============================] - 2s - loss: 0.2446 - val_loss: 0.2413\n",
      "Epoch 228/500\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.244 - 2s - loss: 0.2446 - val_loss: 0.2412\n",
      "Epoch 229/500\n",
      "742/742 [==============================] - 2s - loss: 0.2446 - val_loss: 0.2410\n",
      "Epoch 230/500\n",
      "742/742 [==============================] - 2s - loss: 0.2443 - val_loss: 0.2411\n",
      "Epoch 231/500\n",
      "742/742 [==============================] - 2s - loss: 0.2447 - val_loss: 0.2411\n",
      "Epoch 232/500\n",
      "742/742 [==============================] - 2s - loss: 0.2445 - val_loss: 0.2415\n",
      "Epoch 233/500\n",
      "742/742 [==============================] - 2s - loss: 0.2444 - val_loss: 0.2417ss:\n",
      "Epoch 234/500\n",
      "742/742 [==============================] - 2s - loss: 0.2444 - val_loss: 0.2414\n",
      "Epoch 235/500\n",
      "742/742 [==============================] - 2s - loss: 0.2447 - val_loss: 0.2413\n",
      "Epoch 236/500\n",
      "742/742 [==============================] - 2s - loss: 0.2441 - val_loss: 0.2421\n",
      "Epoch 237/500\n",
      "742/742 [==============================] - 2s - loss: 0.2443 - val_loss: 0.2408\n",
      "Epoch 238/500\n",
      "742/742 [==============================] - 2s - loss: 0.2441 - val_loss: 0.2413\n",
      "Epoch 239/500\n",
      "742/742 [==============================] - 2s - loss: 0.2440 - val_loss: 0.2413\n",
      "Epoch 240/500\n",
      "742/742 [==============================] - 2s - loss: 0.2443 - val_loss: 0.2414\n",
      "Epoch 241/500\n",
      "742/742 [==============================] - 2s - loss: 0.2441 - val_loss: 0.2411\n",
      "Epoch 242/500\n",
      "742/742 [==============================] - 2s - loss: 0.2440 - val_loss: 0.2423\n",
      "Epoch 243/500\n",
      "742/742 [==============================] - 2s - loss: 0.2440 - val_loss: 0.2415\n",
      "Epoch 244/500\n",
      "742/742 [==============================] - 2s - loss: 0.2439 - val_loss: 0.2413\n",
      "Epoch 245/500\n",
      "742/742 [==============================] - 2s - loss: 0.2438 - val_loss: 0.2414\n",
      "Epoch 246/500\n",
      "742/742 [==============================] - 2s - loss: 0.2439 - val_loss: 0.2413\n",
      "Epoch 247/500\n",
      "742/742 [==============================] - 2s - loss: 0.2437 - val_loss: 0.2415\n",
      "Epoch 248/500\n",
      "742/742 [==============================] - 2s - loss: 0.2438 - val_loss: 0.2407\n",
      "Epoch 249/500\n",
      "742/742 [==============================] - 2s - loss: 0.2437 - val_loss: 0.2417\n",
      "Epoch 250/500\n",
      "742/742 [==============================] - 2s - loss: 0.2439 - val_loss: 0.2414\n",
      "Epoch 251/500\n",
      "742/742 [==============================] - 2s - loss: 0.2436 - val_loss: 0.2416\n",
      "Epoch 252/500\n",
      "742/742 [==============================] - 2s - loss: 0.2436 - val_loss: 0.2413\n",
      "Epoch 253/500\n",
      "742/742 [==============================] - 2s - loss: 0.2437 - val_loss: 0.2416\n",
      "Epoch 254/500\n",
      "742/742 [==============================] - 2s - loss: 0.2440 - val_loss: 0.2413\n",
      "Epoch 255/500\n",
      "742/742 [==============================] - 2s - loss: 0.2440 - val_loss: 0.2407\n",
      "Epoch 256/500\n",
      "742/742 [==============================] - 2s - loss: 0.2436 - val_loss: 0.2412\n",
      "Epoch 257/500\n",
      "742/742 [==============================] - 2s - loss: 0.2434 - val_loss: 0.2410\n",
      "Epoch 258/500\n",
      "742/742 [==============================] - 2s - loss: 0.2434 - val_loss: 0.2412\n",
      "Epoch 259/500\n",
      "742/742 [==============================] - 2s - loss: 0.2434 - val_loss: 0.2415\n",
      "Epoch 260/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 2s - loss: 0.2435 - val_loss: 0.2409\n",
      "Epoch 261/500\n",
      "742/742 [==============================] - 2s - loss: 0.2434 - val_loss: 0.2405\n",
      "Epoch 262/500\n",
      "742/742 [==============================] - 2s - loss: 0.2437 - val_loss: 0.2420\n",
      "Epoch 263/500\n",
      "742/742 [==============================] - 2s - loss: 0.2435 - val_loss: 0.2413\n",
      "Epoch 264/500\n",
      "742/742 [==============================] - 2s - loss: 0.2436 - val_loss: 0.2411\n",
      "Epoch 265/500\n",
      "742/742 [==============================] - 2s - loss: 0.2435 - val_loss: 0.2409\n",
      "Epoch 266/500\n",
      "742/742 [==============================] - 3s - loss: 0.2433 - val_loss: 0.2409\n",
      "Epoch 267/500\n",
      "742/742 [==============================] - 2s - loss: 0.2431 - val_loss: 0.2427\n",
      "Epoch 268/500\n",
      "742/742 [==============================] - 6s - loss: 0.2431 - val_loss: 0.2419\n",
      "Epoch 269/500\n",
      "742/742 [==============================] - 3s - loss: 0.2437 - val_loss: 0.2408\n",
      "Epoch 270/500\n",
      "742/742 [==============================] - 2s - loss: 0.2435 - val_loss: 0.2420\n",
      "Epoch 271/500\n",
      "742/742 [==============================] - 2s - loss: 0.2432 - val_loss: 0.2415\n",
      "Epoch 272/500\n",
      "742/742 [==============================] - 2s - loss: 0.2430 - val_loss: 0.2412\n",
      "Epoch 273/500\n",
      "742/742 [==============================] - 2s - loss: 0.2429 - val_loss: 0.2410\n",
      "Epoch 274/500\n",
      "742/742 [==============================] - 2s - loss: 0.2430 - val_loss: 0.2407\n",
      "Epoch 275/500\n",
      "742/742 [==============================] - 2s - loss: 0.2430 - val_loss: 0.2406\n",
      "Epoch 276/500\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.244 - 2s - loss: 0.2432 - val_loss: 0.2420\n",
      "Epoch 277/500\n",
      "742/742 [==============================] - 2s - loss: 0.2435 - val_loss: 0.2407\n",
      "Epoch 278/500\n",
      "742/742 [==============================] - 2s - loss: 0.2430 - val_loss: 0.2410\n",
      "Epoch 279/500\n",
      "742/742 [==============================] - 2s - loss: 0.2430 - val_loss: 0.2409\n",
      "Epoch 280/500\n",
      "742/742 [==============================] - 2s - loss: 0.2428 - val_loss: 0.2410\n",
      "Epoch 281/500\n",
      "742/742 [==============================] - 2s - loss: 0.2428 - val_loss: 0.2412\n",
      "Epoch 282/500\n",
      "742/742 [==============================] - 2s - loss: 0.2431 - val_loss: 0.2417\n",
      "Epoch 283/500\n",
      "742/742 [==============================] - 2s - loss: 0.2429 - val_loss: 0.2415\n",
      "Epoch 284/500\n",
      "742/742 [==============================] - 2s - loss: 0.2428 - val_loss: 0.2416\n",
      "Epoch 285/500\n",
      "742/742 [==============================] - 2s - loss: 0.2427 - val_loss: 0.2407\n",
      "Epoch 286/500\n",
      "742/742 [==============================] - 2s - loss: 0.2427 - val_loss: 0.2408\n",
      "Epoch 287/500\n",
      "742/742 [==============================] - 2s - loss: 0.2429 - val_loss: 0.2408ss: 0.242\n",
      "Epoch 288/500\n",
      "742/742 [==============================] - 2s - loss: 0.2427 - val_loss: 0.2405\n",
      "Epoch 289/500\n",
      "742/742 [==============================] - 2s - loss: 0.2431 - val_loss: 0.2415\n",
      "Epoch 290/500\n",
      "742/742 [==============================] - 2s - loss: 0.2430 - val_loss: 0.2404\n",
      "Epoch 291/500\n",
      "742/742 [==============================] - 2s - loss: 0.2428 - val_loss: 0.2410\n",
      "Epoch 292/500\n",
      "742/742 [==============================] - 2s - loss: 0.2425 - val_loss: 0.2406ss: 0.24\n",
      "Epoch 293/500\n",
      "742/742 [==============================] - 2s - loss: 0.2424 - val_loss: 0.2409\n",
      "Epoch 294/500\n",
      "742/742 [==============================] - 2s - loss: 0.2425 - val_loss: 0.2407\n",
      "Epoch 295/500\n",
      "742/742 [==============================] - 3s - loss: 0.2425 - val_loss: 0.2411\n",
      "Epoch 296/500\n",
      "742/742 [==============================] - 2s - loss: 0.2423 - val_loss: 0.2414\n",
      "Epoch 297/500\n",
      "742/742 [==============================] - 2s - loss: 0.2423 - val_loss: 0.2417\n",
      "Epoch 298/500\n",
      "742/742 [==============================] - 2s - loss: 0.2428 - val_loss: 0.2410\n",
      "Epoch 299/500\n",
      "742/742 [==============================] - 2s - loss: 0.2431 - val_loss: 0.2421\n",
      "Epoch 300/500\n",
      "742/742 [==============================] - 2s - loss: 0.2428 - val_loss: 0.2417\n",
      "Epoch 301/500\n",
      "742/742 [==============================] - 2s - loss: 0.2426 - val_loss: 0.2418\n",
      "Epoch 302/500\n",
      "742/742 [==============================] - 2s - loss: 0.2425 - val_loss: 0.2424\n",
      "Epoch 303/500\n",
      "742/742 [==============================] - 2s - loss: 0.2424 - val_loss: 0.2404ss: \n",
      "Epoch 304/500\n",
      "742/742 [==============================] - 2s - loss: 0.2423 - val_loss: 0.2401\n",
      "Epoch 305/500\n",
      "742/742 [==============================] - 2s - loss: 0.2423 - val_loss: 0.2410\n",
      "Epoch 306/500\n",
      "742/742 [==============================] - 2s - loss: 0.2422 - val_loss: 0.2406\n",
      "Epoch 307/500\n",
      "742/742 [==============================] - 2s - loss: 0.2424 - val_loss: 0.2406\n",
      "Epoch 308/500\n",
      "742/742 [==============================] - 7s - loss: 0.2427 - val_loss: 0.2409\n",
      "Epoch 309/500\n",
      "742/742 [==============================] - 2s - loss: 0.2422 - val_loss: 0.2408\n",
      "Epoch 310/500\n",
      "742/742 [==============================] - 2s - loss: 0.2421 - val_loss: 0.2406\n",
      "Epoch 311/500\n",
      "742/742 [==============================] - 2s - loss: 0.2420 - val_loss: 0.2401\n",
      "Epoch 312/500\n",
      "742/742 [==============================] - 2s - loss: 0.2419 - val_loss: 0.2402\n",
      "Epoch 313/500\n",
      "742/742 [==============================] - 2s - loss: 0.2421 - val_loss: 0.2410\n",
      "Epoch 314/500\n",
      "742/742 [==============================] - 2s - loss: 0.2419 - val_loss: 0.2405\n",
      "Epoch 315/500\n",
      "742/742 [==============================] - 2s - loss: 0.2419 - val_loss: 0.2406\n",
      "Epoch 316/500\n",
      "742/742 [==============================] - 2s - loss: 0.2420 - val_loss: 0.2407\n",
      "Epoch 317/500\n",
      "742/742 [==============================] - 2s - loss: 0.2423 - val_loss: 0.2408\n",
      "Epoch 318/500\n",
      "742/742 [==============================] - 2s - loss: 0.2420 - val_loss: 0.2404\n",
      "Epoch 319/500\n",
      "742/742 [==============================] - 5s - loss: 0.2419 - val_loss: 0.2405\n",
      "Epoch 320/500\n",
      "742/742 [==============================] - 4s - loss: 0.2417 - val_loss: 0.2406\n",
      "Epoch 321/500\n",
      "742/742 [==============================] - 2s - loss: 0.2419 - val_loss: 0.2410\n",
      "Epoch 322/500\n",
      "742/742 [==============================] - 2s - loss: 0.2419 - val_loss: 0.2405\n",
      "Epoch 323/500\n",
      "742/742 [==============================] - 2s - loss: 0.2418 - val_loss: 0.2407\n",
      "Epoch 324/500\n",
      "742/742 [==============================] - 2s - loss: 0.2416 - val_loss: 0.2400\n",
      "Epoch 325/500\n",
      "742/742 [==============================] - 2s - loss: 0.2417 - val_loss: 0.2406\n",
      "Epoch 326/500\n",
      "742/742 [==============================] - 2s - loss: 0.2417 - val_loss: 0.2410\n",
      "Epoch 327/500\n",
      "742/742 [==============================] - 2s - loss: 0.2418 - val_loss: 0.2404\n",
      "Epoch 328/500\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.242 - 2s - loss: 0.2416 - val_loss: 0.2408\n",
      "Epoch 329/500\n",
      "742/742 [==============================] - 2s - loss: 0.2416 - val_loss: 0.2404\n",
      "Epoch 330/500\n",
      "742/742 [==============================] - 2s - loss: 0.2418 - val_loss: 0.2401\n",
      "Epoch 331/500\n",
      "742/742 [==============================] - 2s - loss: 0.2418 - val_loss: 0.2410\n",
      "Epoch 332/500\n",
      "742/742 [==============================] - 2s - loss: 0.2415 - val_loss: 0.2403\n",
      "Epoch 333/500\n",
      "742/742 [==============================] - 2s - loss: 0.2414 - val_loss: 0.2399\n",
      "Epoch 334/500\n",
      "742/742 [==============================] - 2s - loss: 0.2416 - val_loss: 0.2398\n",
      "Epoch 335/500\n",
      "742/742 [==============================] - 6s - loss: 0.2416 - val_loss: 0.2413\n",
      "Epoch 336/500\n",
      "742/742 [==============================] - 3s - loss: 0.2418 - val_loss: 0.2399\n",
      "Epoch 337/500\n",
      "742/742 [==============================] - 2s - loss: 0.2417 - val_loss: 0.2407\n",
      "Epoch 338/500\n",
      "742/742 [==============================] - 2s - loss: 0.2416 - val_loss: 0.2410\n",
      "Epoch 339/500\n",
      "742/742 [==============================] - 2s - loss: 0.2415 - val_loss: 0.2436\n",
      "Epoch 340/500\n",
      "742/742 [==============================] - 2s - loss: 0.2415 - val_loss: 0.2405\n",
      "Epoch 341/500\n",
      "742/742 [==============================] - 2s - loss: 0.2413 - val_loss: 0.2413\n",
      "Epoch 342/500\n",
      "742/742 [==============================] - 2s - loss: 0.2415 - val_loss: 0.2405\n",
      "Epoch 343/500\n",
      "742/742 [==============================] - 2s - loss: 0.2414 - val_loss: 0.2410\n",
      "Epoch 344/500\n",
      "742/742 [==============================] - 2s - loss: 0.2415 - val_loss: 0.2405\n",
      "Epoch 345/500\n",
      "742/742 [==============================] - 2s - loss: 0.2413 - val_loss: 0.2409\n",
      "Epoch 346/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 2s - loss: 0.2412 - val_loss: 0.2406\n",
      "Epoch 347/500\n",
      "742/742 [==============================] - 2s - loss: 0.2411 - val_loss: 0.2404\n",
      "Epoch 348/500\n",
      "742/742 [==============================] - 2s - loss: 0.2418 - val_loss: 0.2399\n",
      "Epoch 349/500\n",
      "742/742 [==============================] - 2s - loss: 0.2414 - val_loss: 0.2406\n",
      "Epoch 350/500\n",
      "742/742 [==============================] - 2s - loss: 0.2411 - val_loss: 0.2406\n",
      "Epoch 351/500\n",
      "742/742 [==============================] - 2s - loss: 0.2409 - val_loss: 0.2402\n",
      "Epoch 352/500\n",
      "742/742 [==============================] - 2s - loss: 0.2411 - val_loss: 0.2402\n",
      "Epoch 353/500\n",
      "742/742 [==============================] - 2s - loss: 0.2410 - val_loss: 0.2401\n",
      "Epoch 354/500\n",
      "742/742 [==============================] - 2s - loss: 0.2412 - val_loss: 0.2404\n",
      "Epoch 355/500\n",
      "742/742 [==============================] - 6s - loss: 0.2411 - val_loss: 0.2399\n",
      "Epoch 356/500\n",
      "742/742 [==============================] - 2s - loss: 0.2414 - val_loss: 0.2405\n",
      "Epoch 357/500\n",
      "742/742 [==============================] - 2s - loss: 0.2415 - val_loss: 0.2400\n",
      "Epoch 358/500\n",
      "742/742 [==============================] - 2s - loss: 0.2412 - val_loss: 0.2406\n",
      "Epoch 359/500\n",
      "742/742 [==============================] - 2s - loss: 0.2413 - val_loss: 0.2407\n",
      "Epoch 360/500\n",
      "742/742 [==============================] - 3s - loss: 0.2410 - val_loss: 0.2406\n",
      "Epoch 361/500\n",
      "742/742 [==============================] - 2s - loss: 0.2410 - val_loss: 0.2399\n",
      "Epoch 362/500\n",
      "742/742 [==============================] - 2s - loss: 0.2410 - val_loss: 0.2412\n",
      "Epoch 363/500\n",
      "742/742 [==============================] - 2s - loss: 0.2409 - val_loss: 0.2396\n",
      "Epoch 364/500\n",
      "742/742 [==============================] - 2s - loss: 0.2413 - val_loss: 0.2404\n",
      "Epoch 365/500\n",
      "742/742 [==============================] - 2s - loss: 0.2415 - val_loss: 0.2397\n",
      "Epoch 366/500\n",
      "742/742 [==============================] - 2s - loss: 0.2411 - val_loss: 0.2404\n",
      "Epoch 367/500\n",
      "742/742 [==============================] - 2s - loss: 0.2408 - val_loss: 0.2404\n",
      "Epoch 368/500\n",
      "742/742 [==============================] - 2s - loss: 0.2407 - val_loss: 0.2397\n",
      "Epoch 369/500\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.240 - 2s - loss: 0.2407 - val_loss: 0.2399\n",
      "Epoch 370/500\n",
      "742/742 [==============================] - 2s - loss: 0.2408 - val_loss: 0.2395\n",
      "Epoch 371/500\n",
      "742/742 [==============================] - 2s - loss: 0.2409 - val_loss: 0.2408\n",
      "Epoch 372/500\n",
      "742/742 [==============================] - 2s - loss: 0.2409 - val_loss: 0.2399\n",
      "Epoch 373/500\n",
      "742/742 [==============================] - 2s - loss: 0.2412 - val_loss: 0.2395\n",
      "Epoch 374/500\n",
      "742/742 [==============================] - 2s - loss: 0.2407 - val_loss: 0.2401\n",
      "Epoch 375/500\n",
      "742/742 [==============================] - 2s - loss: 0.2408 - val_loss: 0.2406\n",
      "Epoch 376/500\n",
      "742/742 [==============================] - 2s - loss: 0.2407 - val_loss: 0.2400\n",
      "Epoch 377/500\n",
      "742/742 [==============================] - 3s - loss: 0.2407 - val_loss: 0.2395\n",
      "Epoch 378/500\n",
      "742/742 [==============================] - 7s - loss: 0.2408 - val_loss: 0.2404\n",
      "Epoch 379/500\n",
      "742/742 [==============================] - 2s - loss: 0.2406 - val_loss: 0.2402\n",
      "Epoch 380/500\n",
      "742/742 [==============================] - 2s - loss: 0.2407 - val_loss: 0.2403s\n",
      "Epoch 381/500\n",
      "742/742 [==============================] - 2s - loss: 0.2405 - val_loss: 0.2404\n",
      "Epoch 382/500\n",
      "742/742 [==============================] - 2s - loss: 0.2406 - val_loss: 0.2399\n",
      "Epoch 383/500\n",
      "742/742 [==============================] - 2s - loss: 0.2407 - val_loss: 0.2396\n",
      "Epoch 384/500\n",
      "742/742 [==============================] - 2s - loss: 0.2405 - val_loss: 0.2395\n",
      "Epoch 385/500\n",
      "742/742 [==============================] - 2s - loss: 0.2406 - val_loss: 0.2391\n",
      "Epoch 386/500\n",
      "742/742 [==============================] - 2s - loss: 0.2409 - val_loss: 0.2391\n",
      "Epoch 387/500\n",
      "742/742 [==============================] - 2s - loss: 0.2412 - val_loss: 0.2397\n",
      "Epoch 388/500\n",
      "742/742 [==============================] - 2s - loss: 0.2407 - val_loss: 0.2417\n",
      "Epoch 389/500\n",
      "742/742 [==============================] - 2s - loss: 0.2406 - val_loss: 0.2398\n",
      "Epoch 390/500\n",
      "742/742 [==============================] - 2s - loss: 0.2403 - val_loss: 0.2407\n",
      "Epoch 391/500\n",
      "742/742 [==============================] - 3s - loss: 0.2403 - val_loss: 0.2395\n",
      "Epoch 392/500\n",
      "742/742 [==============================] - 6s - loss: 0.2404 - val_loss: 0.2403\n",
      "Epoch 393/500\n",
      "742/742 [==============================] - 2s - loss: 0.2405 - val_loss: 0.2404\n",
      "Epoch 394/500\n",
      "742/742 [==============================] - 3s - loss: 0.2404 - val_loss: 0.2396\n",
      "Epoch 395/500\n",
      "742/742 [==============================] - 2s - loss: 0.2404 - val_loss: 0.2396\n",
      "Epoch 396/500\n",
      "742/742 [==============================] - 2s - loss: 0.2407 - val_loss: 0.2407\n",
      "Epoch 397/500\n",
      "742/742 [==============================] - 2s - loss: 0.2411 - val_loss: 0.2410\n",
      "Epoch 398/500\n",
      "742/742 [==============================] - 2s - loss: 0.2407 - val_loss: 0.2397\n",
      "Epoch 399/500\n",
      "742/742 [==============================] - 2s - loss: 0.2403 - val_loss: 0.2396\n",
      "Epoch 400/500\n",
      "742/742 [==============================] - 2s - loss: 0.2400 - val_loss: 0.2412\n",
      "Epoch 401/500\n",
      "742/742 [==============================] - 2s - loss: 0.2402 - val_loss: 0.2403\n",
      "Epoch 402/500\n",
      "742/742 [==============================] - 2s - loss: 0.2401 - val_loss: 0.2401ss: 0.\n",
      "Epoch 403/500\n",
      "742/742 [==============================] - 2s - loss: 0.2401 - val_loss: 0.2392\n",
      "Epoch 404/500\n",
      "742/742 [==============================] - 6s - loss: 0.2401 - val_loss: 0.2391\n",
      "Epoch 405/500\n",
      "742/742 [==============================] - 2s - loss: 0.2406 - val_loss: 0.2400\n",
      "Epoch 406/500\n",
      "742/742 [==============================] - 2s - loss: 0.2401 - val_loss: 0.2402\n",
      "Epoch 407/500\n",
      "742/742 [==============================] - 2s - loss: 0.2399 - val_loss: 0.2394\n",
      "Epoch 408/500\n",
      "742/742 [==============================] - 2s - loss: 0.2404 - val_loss: 0.2402\n",
      "Epoch 409/500\n",
      "742/742 [==============================] - 2s - loss: 0.2402 - val_loss: 0.2398\n",
      "Epoch 410/500\n",
      "742/742 [==============================] - 2s - loss: 0.2400 - val_loss: 0.2397\n",
      "Epoch 411/500\n",
      "742/742 [==============================] - 2s - loss: 0.2401 - val_loss: 0.2397\n",
      "Epoch 412/500\n",
      "742/742 [==============================] - 2s - loss: 0.2399 - val_loss: 0.2402\n",
      "Epoch 413/500\n",
      "742/742 [==============================] - 2s - loss: 0.2405 - val_loss: 0.2406\n",
      "Epoch 414/500\n",
      "742/742 [==============================] - 2s - loss: 0.2404 - val_loss: 0.2400\n",
      "Epoch 415/500\n",
      "742/742 [==============================] - 2s - loss: 0.2403 - val_loss: 0.2400\n",
      "Epoch 416/500\n",
      "742/742 [==============================] - 2s - loss: 0.2400 - val_loss: 0.2401\n",
      "Epoch 417/500\n",
      "742/742 [==============================] - 2s - loss: 0.2397 - val_loss: 0.2404\n",
      "Epoch 418/500\n",
      "742/742 [==============================] - 2s - loss: 0.2397 - val_loss: 0.2394\n",
      "Epoch 419/500\n",
      "742/742 [==============================] - 2s - loss: 0.2397 - val_loss: 0.2410\n",
      "Epoch 420/500\n",
      "742/742 [==============================] - 2s - loss: 0.2398 - val_loss: 0.2399\n",
      "Epoch 421/500\n",
      "742/742 [==============================] - 6s - loss: 0.2397 - val_loss: 0.2401\n",
      "Epoch 422/500\n",
      "742/742 [==============================] - 3s - loss: 0.2398 - val_loss: 0.2401\n",
      "Epoch 423/500\n",
      "742/742 [==============================] - 2s - loss: 0.2397 - val_loss: 0.2397\n",
      "Epoch 424/500\n",
      "742/742 [==============================] - 2s - loss: 0.2396 - val_loss: 0.2390\n",
      "Epoch 425/500\n",
      "742/742 [==============================] - 2s - loss: 0.2398 - val_loss: 0.2406\n",
      "Epoch 426/500\n",
      "742/742 [==============================] - 3s - loss: 0.2400 - val_loss: 0.2394\n",
      "Epoch 427/500\n",
      "742/742 [==============================] - 3s - loss: 0.2397 - val_loss: 0.2392\n",
      "Epoch 428/500\n",
      "742/742 [==============================] - 3s - loss: 0.2397 - val_loss: 0.2396\n",
      "Epoch 429/500\n",
      "742/742 [==============================] - 2s - loss: 0.2397 - val_loss: 0.2399\n",
      "Epoch 430/500\n",
      "742/742 [==============================] - 2s - loss: 0.2396 - val_loss: 0.2391\n",
      "Epoch 431/500\n",
      "742/742 [==============================] - 3s - loss: 0.2396 - val_loss: 0.2400\n",
      "Epoch 432/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 2s - loss: 0.2395 - val_loss: 0.2403s\n",
      "Epoch 433/500\n",
      "742/742 [==============================] - 3s - loss: 0.2397 - val_loss: 0.2392\n",
      "Epoch 434/500\n",
      "742/742 [==============================] - 2s - loss: 0.2396 - val_loss: 0.2400\n",
      "Epoch 435/500\n",
      "742/742 [==============================] - 3s - loss: 0.2397 - val_loss: 0.2395\n",
      "Epoch 436/500\n",
      "742/742 [==============================] - 3s - loss: 0.2403 - val_loss: 0.2396\n",
      "Epoch 437/500\n",
      "742/742 [==============================] - 3s - loss: 0.2399 - val_loss: 0.2401\n",
      "Epoch 438/500\n",
      "742/742 [==============================] - 2s - loss: 0.2399 - val_loss: 0.2390\n",
      "Epoch 439/500\n",
      "742/742 [==============================] - 2s - loss: 0.2395 - val_loss: 0.2396\n",
      "Epoch 440/500\n",
      "742/742 [==============================] - 3s - loss: 0.2394 - val_loss: 0.2397\n",
      "Epoch 441/500\n",
      "742/742 [==============================] - 3s - loss: 0.2396 - val_loss: 0.2392\n",
      "Epoch 442/500\n",
      "742/742 [==============================] - 2s - loss: 0.2396 - val_loss: 0.2394\n",
      "Epoch 443/500\n",
      "742/742 [==============================] - 2s - loss: 0.2394 - val_loss: 0.2401\n",
      "Epoch 444/500\n",
      "742/742 [==============================] - 2s - loss: 0.2395 - val_loss: 0.2400\n",
      "Epoch 445/500\n",
      "742/742 [==============================] - 7s - loss: 0.2393 - val_loss: 0.2400\n",
      "Epoch 446/500\n",
      "742/742 [==============================] - 2s - loss: 0.2393 - val_loss: 0.2390\n",
      "Epoch 447/500\n",
      "742/742 [==============================] - 2s - loss: 0.2395 - val_loss: 0.2397\n",
      "Epoch 448/500\n",
      "742/742 [==============================] - 2s - loss: 0.2395 - val_loss: 0.2395\n",
      "Epoch 449/500\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.237 - 2s - loss: 0.2392 - val_loss: 0.2390\n",
      "Epoch 450/500\n",
      "742/742 [==============================] - 2s - loss: 0.2392 - val_loss: 0.2397\n",
      "Epoch 451/500\n",
      "742/742 [==============================] - 2s - loss: 0.2394 - val_loss: 0.2389\n",
      "Epoch 452/500\n",
      "742/742 [==============================] - 2s - loss: 0.2398 - val_loss: 0.2401\n",
      "Epoch 453/500\n",
      "742/742 [==============================] - 2s - loss: 0.2396 - val_loss: 0.2400\n",
      "Epoch 454/500\n",
      "742/742 [==============================] - 2s - loss: 0.2392 - val_loss: 0.2404\n",
      "Epoch 455/500\n",
      "742/742 [==============================] - 2s - loss: 0.2394 - val_loss: 0.2400\n",
      "Epoch 456/500\n",
      "742/742 [==============================] - 2s - loss: 0.2392 - val_loss: 0.2393\n",
      "Epoch 457/500\n",
      "742/742 [==============================] - 2s - loss: 0.2391 - val_loss: 0.2395\n",
      "Epoch 458/500\n",
      "742/742 [==============================] - 2s - loss: 0.2391 - val_loss: 0.2396\n",
      "Epoch 459/500\n",
      "742/742 [==============================] - 3s - loss: 0.2392 - val_loss: 0.2392\n",
      "Epoch 460/500\n",
      "742/742 [==============================] - 3s - loss: 0.2393 - val_loss: 0.2398\n",
      "Epoch 461/500\n",
      "742/742 [==============================] - 6s - loss: 0.2391 - val_loss: 0.2405\n",
      "Epoch 462/500\n",
      "742/742 [==============================] - 5s - loss: 0.2391 - val_loss: 0.2398\n",
      "Epoch 463/500\n",
      "742/742 [==============================] - 2s - loss: 0.2390 - val_loss: 0.2388\n",
      "Epoch 464/500\n",
      "742/742 [==============================] - 3s - loss: 0.2390 - val_loss: 0.2392\n",
      "Epoch 465/500\n",
      "742/742 [==============================] - 2s - loss: 0.2391 - val_loss: 0.2392\n",
      "Epoch 466/500\n",
      "742/742 [==============================] - 2s - loss: 0.2392 - val_loss: 0.2392\n",
      "Epoch 467/500\n",
      "742/742 [==============================] - 2s - loss: 0.2391 - val_loss: 0.2401\n",
      "Epoch 468/500\n",
      "742/742 [==============================] - 2s - loss: 0.2390 - val_loss: 0.2401\n",
      "Epoch 469/500\n",
      "742/742 [==============================] - 2s - loss: 0.2390 - val_loss: 0.2400\n",
      "Epoch 470/500\n",
      "742/742 [==============================] - 2s - loss: 0.2389 - val_loss: 0.2394\n",
      "Epoch 471/500\n",
      "742/742 [==============================] - 2s - loss: 0.2389 - val_loss: 0.2395\n",
      "Epoch 472/500\n",
      "742/742 [==============================] - 2s - loss: 0.2388 - val_loss: 0.2392\n",
      "Epoch 473/500\n",
      "742/742 [==============================] - 2s - loss: 0.2388 - val_loss: 0.2397\n",
      "Epoch 474/500\n",
      "742/742 [==============================] - 2s - loss: 0.2391 - val_loss: 0.2403\n",
      "Epoch 475/500\n",
      "742/742 [==============================] - 2s - loss: 0.2393 - val_loss: 0.2391\n",
      "Epoch 476/500\n",
      "742/742 [==============================] - 2s - loss: 0.2392 - val_loss: 0.2407\n",
      "Epoch 477/500\n",
      "742/742 [==============================] - 2s - loss: 0.2393 - val_loss: 0.2398\n",
      "Epoch 478/500\n",
      "742/742 [==============================] - 2s - loss: 0.2389 - val_loss: 0.2393\n",
      "Epoch 479/500\n",
      "742/742 [==============================] - 9s - loss: 0.2391 - val_loss: 0.2391\n",
      "Epoch 480/500\n",
      "742/742 [==============================] - 3s - loss: 0.2393 - val_loss: 0.2392\n",
      "Epoch 481/500\n",
      "742/742 [==============================] - 2s - loss: 0.2390 - val_loss: 0.2394\n",
      "Epoch 482/500\n",
      "742/742 [==============================] - 2s - loss: 0.2388 - val_loss: 0.2399\n",
      "Epoch 483/500\n",
      "742/742 [==============================] - 2s - loss: 0.2390 - val_loss: 0.2392ss\n",
      "Epoch 484/500\n",
      "742/742 [==============================] - 2s - loss: 0.2394 - val_loss: 0.2399\n",
      "Epoch 485/500\n",
      "742/742 [==============================] - 2s - loss: 0.2391 - val_loss: 0.2402\n",
      "Epoch 486/500\n",
      "742/742 [==============================] - 2s - loss: 0.2388 - val_loss: 0.2395\n",
      "Epoch 487/500\n",
      "742/742 [==============================] - 2s - loss: 0.2388 - val_loss: 0.2389\n",
      "Epoch 488/500\n",
      "742/742 [==============================] - 3s - loss: 0.2387 - val_loss: 0.2391\n",
      "Epoch 489/500\n",
      "742/742 [==============================] - 2s - loss: 0.2386 - val_loss: 0.2397\n",
      "Epoch 490/500\n",
      "742/742 [==============================] - 2s - loss: 0.2387 - val_loss: 0.2392\n",
      "Epoch 491/500\n",
      "742/742 [==============================] - 2s - loss: 0.2387 - val_loss: 0.2396\n",
      "Epoch 492/500\n",
      "742/742 [==============================] - 2s - loss: 0.2387 - val_loss: 0.2389\n",
      "Epoch 493/500\n",
      "742/742 [==============================] - 2s - loss: 0.2387 - val_loss: 0.2391\n",
      "Epoch 494/500\n",
      "742/742 [==============================] - 2s - loss: 0.2388 - val_loss: 0.2395\n",
      "Epoch 495/500\n",
      "742/742 [==============================] - 2s - loss: 0.2386 - val_loss: 0.2393\n",
      "Epoch 496/500\n",
      "742/742 [==============================] - 2s - loss: 0.2385 - val_loss: 0.2388\n",
      "Epoch 497/500\n",
      "742/742 [==============================] - 2s - loss: 0.2389 - val_loss: 0.2393\n",
      "Epoch 498/500\n",
      "742/742 [==============================] - 2s - loss: 0.2386 - val_loss: 0.2388\n",
      "Epoch 499/500\n",
      "742/742 [==============================] - 2s - loss: 0.2390 - val_loss: 0.2394\n",
      "Epoch 500/500\n",
      "742/742 [==============================] - 2s - loss: 0.2387 - val_loss: 0.2398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f06ef60>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train,x_train,epochs=500,batch_size=64,shuffle=True,validation_data=(x_test,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=Model(input_img,encoded)\n",
    "\n",
    "encoded_input=Input(shape=(5, 5, 16))\n",
    "decoder_layer=encoded_input\n",
    "for i in range(5):\n",
    "    temp=autoencoder.layers[i-5]\n",
    "    decoder_layer=temp(decoder_layer)\n",
    "\n",
    "decoder=Model(encoded_input,decoder_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAADlCAYAAABK1oajAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIVptAAAAArTaAEAABT2E1LUH3Y2DYqPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f1f6c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_imgs = encoder.predict(x_train)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "n = 8  # how many digits we will display\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    imgplot = plt.imshow(x_train[i,:,:,0])\n",
    "    #plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    imgplot = plt.imshow(decoded_imgs[i,:,:,0])\n",
    "    #plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
